{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.dii.uchile.cl/wp-content/uploads/2021/06/Magi%CC%81ster-en-Ciencia-de-Datos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proyecto 1 - MDS7202 Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos üìö**\n",
    "\n",
    "**MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos**\n",
    "\n",
    "### Cuerpo Docente:\n",
    "\n",
    "- Profesor: Ignacio Meza, Gabriel Iturra\n",
    "- Auxiliar: Sebasti√°n Tinoco\n",
    "- Ayudante: Arturo Lazcano, Angelo Mu√±oz\n",
    "\n",
    "*Por favor, lean detalladamente las instrucciones de la tarea antes de empezar a escribir.*\n",
    "\n",
    "### Equipo:\n",
    "\n",
    "- Tom√°s Aguirre\n",
    "- Ignacio Albornoz\n",
    "\n",
    "\n",
    "### Link de repositorio de GitHub: https://github.com/tomasaguirre-ignacioalbornoz/MDS7202\n",
    "\n",
    "Fecha l√≠mite de entrega üìÜ: 27 de Octubre de 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Reglas\n",
    "\n",
    "- **Grupos de 2 personas.**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
    "- Estrictamente prohibida la copia. \n",
    "- Pueden usar cualquier material del curso que estimen conveniente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://worldskateamerica.org/wp-content/uploads/2023/07/SANTIAGO-2023-1-768x153.jpg\" alt=\"Descripci√≥n de la imagen\">\n",
    "</div>\n",
    "\n",
    "En un Chile azotado por un profundo caos pol√≠tico-econ√≥mico y el resurgimiento de programas de televisi√≥n de dudosa calidad, todas las miradas y esperanzas son depositadas en el √©xito de un √∫nico evento: Santiago 2023. La naci√≥n necesitaba desesperadamente un respiro, y los Juegos de Santiago 2023 promet√≠an ser una luz al final del t√∫nel.\n",
    "\n",
    "El Presidente de la Rep√∫blica -conocido en las calles como Bomb√≠n-, consciente de la importancia de este evento para la revitalizaci√≥n del pa√≠s, decide convocar a usted y su equipo en calidad de expertos en an√°lisis de datos y estad√≠sticas. Con gran solemnidad, el presidente les encomienda una importante y peligrosa: liderar un proyecto que permitiera caracterizar de forma autom√°tica y eficiente los datos generados por estos magnos juegos. Para esto, el presidente le destaca que la soluci√≥n debe considerar los siguientes puntos:\n",
    "- Caracterizaci√≥n autom√°tica de los datos\n",
    "- La soluci√≥n debe ser compatible con cualquier dataset\n",
    "- Se les facilita el dataset *olimpiadas.parquet*, el cual recopila data de diferentes juegos ol√≠mpicos realizados en los √∫ltimos a√±os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Creaci√≥n de `Profiler` Class (4.0 puntos)\n",
    "\n",
    "Cree la clase `Profiler`. Como m√≠nimo, esta debe tener las siguientes funcionalidades:\n",
    "\n",
    "1. El m√©todo constructor, el cual debe recibir los datos a procesar en formato `Pandas DataFrame`. Adem√°s, este m√©todo debe generar una carpeta en su directorio de trabajo con el nombre `EDA_fecha`, donde `fecha` corresponda a la fecha de ejecuci√≥n en formato `DD-MM-YYYY`.\n",
    "\n",
    "2. El m√©todo `summarize`, el cual debe caracterizar las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    - Reportar el tipo de variable\n",
    "    - Reportar el n√∫mero y/o porcentaje de valores √∫nicos de la variable\n",
    "    - Reportar el n√∫mero y/o porcentaje de valores nulos\n",
    "    - Si la variables es num√©rica:\n",
    "        - Reportar el n√∫mero y/o porcentaje de valores cero, negativos y outliers\n",
    "        - Reportar estad√≠stica descriptiva como el valor m√≠nimo, m√°ximo, promedio y los percentiles 25, 50, 75 y 100\n",
    "   - Levantar una alerta en caso de encontrar alguna anomal√≠a fuera de lo com√∫n (el criterio debe ser ajustable por el usuario)\n",
    "   - Guardar sus resultados en el directorio `EDA_fecha/summary.txt`. El archivo debe separar de forma clara y ordenada los resultados de cada punto.\n",
    "\n",
    "3. El m√©todo `plot_vars`, el cual debe graficar la distribuci√≥n e interraciones de las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/plots`\n",
    "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    - Para las variables num√©ricas:\n",
    "        - Genere un gr√°fico de distribuci√≥n de densidad\n",
    "        - Grafique la correlaci√≥n entre las variables\n",
    "    - Para las variables categ√≥ricas:\n",
    "        - Genere un histograma de las top N categor√≠as (N debe ser un par√°metro ajustable)\n",
    "        - Grafique el coeficiente V de Cramer entre las variables\n",
    "    - Guardar cada gr√°fico generado en la carpeta `EDA_fecha/plots` en formato `.pdf` y bajo el naming `variable.pdf`, donde `variable` es el nombre de la variable de inter√©s\n",
    "    \n",
    "4. El m√©todo `clean_data`, el cual debe limpiar los datos para que luego puedan ser procesados. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/clean_data`\n",
    "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    - Drop de valores duplicados\n",
    "    - Implementar como m√≠nimo 2 t√©cnicas para tratar los valores nulos, como:\n",
    "        - Drop de valores nulos\n",
    "        - Imputar valores nulos con alguna t√©cnica de imputaci√≥n\n",
    "        - Funcionalidad para escoger entre una t√©cnica y la otra.\n",
    "    - Una de las columnas del dataframe presenta datos *no at√≥micos*. Separe dicha columna en las columnas que la compongan.\n",
    "        - Hint: ¬øQu√© caracteres permiten separar una columna de otra?\n",
    "        - Para las pruebas con el dataset nuevo, puede esperar que exista al menos una columna con este tipo de problema. Asuma que los separadores ser√°n los mismos, aunque el n√∫mero de columnas a separar puede ser distinto.\n",
    "    - Deber√≠an usar `FunctionTransformer`.\n",
    "    - Guardar los datos procesados en formato `.csv` en el path `EDA_fecha/clean_data/data.csv`\n",
    "\n",
    "5. El m√©todo `scale`, el cual debe preparar adecuadamente los datos para luego ser consumidos por alg√∫n tipo de algoritmo. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/scale`\n",
    "    - Procesar de forma adecuada los datos num√©ricos y categ√≥ricos:\n",
    "        - Su m√©todo debe recibir las t√©cnicas de escalamiento como argumento de entrada (utilizar solo t√©cnicas compatibles con el framework de `sklearn`)\n",
    "        - Para los atributos num√©ricos, se transforme los datos con un escalador logar√≠tmico y un `MinMaxScaler`\n",
    "        - Asuma que no existen datos ordinales en su dataset\n",
    "    - Guardar todo este procesamiento en un `ColumnTransformer`.\n",
    "    - Guardar los datos limpios y transformados en formato `.csv` en el path `EDA_fecha/process/scaled_features.csv`\n",
    "\n",
    "6. El m√©todo `make_clusters`, el cual debe generar clusters de los datos usando alg√∫n algoritmo de clusterizaci√≥n. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/clusters`\n",
    "    - Generar un estudio del codo donde se√±ale la cantidad de clusters optimos para el desarrollo.\n",
    "    - Su m√©todo debe recibir el algoritmo de clustering como argumento de entrada (utilizar solo algoritmos compatibles con el framework de `sklearn`).\n",
    "    - No olvide pre procesar adecuadamente los datos antes de implementar la t√©cnica de clustering. \n",
    "    - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5. \n",
    "    - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
    "    - Una vez generado los clusters, proyecte los datos a 2 dimensiones usando su t√©cnica de reducci√≥n de dimensionalidad favorita y grafique los resultados coloreando por cluster.\n",
    "    - Guardar los datos con su respectivo cluster en formato `.csv` en el path `EDA_fecha/clusters/data_clusters.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
    "\n",
    "7. El m√©todo `detect_anomalies`, el cual debe detectar anomal√≠as en los datos. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "\n",
    "    - Crear la carpeta `EDA_fecha/anomalies`\n",
    "    - Implementar alguna t√©cnica de detecci√≥n de anomal√≠as.\n",
    "    - Al igual que el punto anterior, su m√©todo debe considerar los siguientes puntos:\n",
    "        - No olvide pre procesar de forma adecuada los datos antes de implementar la t√©cnica de detecci√≥n de anomal√≠a. \n",
    "        - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5. \n",
    "        - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
    "        - Su m√©todo debe recibir el algoritmo como argumento de entrada\n",
    "        - Una vez generado las etiquetas, proyecte los datos a 2 dimensiones y grafique los resultados coloreando por las etiquetas predichas por el detector de anomal√≠as\n",
    "    - Guardar los datos con su respectiva etiqueta en formato `.csv` en el path `EDA_fecha/anomalies/data_anomalies.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
    "\n",
    "8. El m√©todo `profile`, el cual debe ejecutar todos los m√©todos anteriores.\n",
    "\n",
    "9. Crear el m√©todo `clearGarbage` para eliminar las carpetas/archivos creados/as por la clase `Profiler`.\n",
    "\n",
    "Algunas consideraciones generales:\n",
    "- Su clase ser√° testeada con datos tabulares diferentes a los provistos. No desarrollen c√≥digo *hardcodeado*: su clase debe ser capaz de funcionar para **cualquier** dataset. \n",
    "- Aplique todo su conocimiento sobre buenas pr√°cticas de programaci√≥n: se evaluar√° que su c√≥digo sea limpio y ordenado.\n",
    "- Recuerden documentar cada una de las funcionalidades que implementen.\n",
    "- Recuerden adjuntar sus `requirements.txt` junto a su entrega de proyecto. **El c√≥digo que no se pueda ejecutar por imcompatibilidades de librer√≠as no ser√° corregido.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "from collections import Counter\n",
    "import shutil\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.preprocessing import FunctionTransformer, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "df = pd.read_parquet('olimpiadas.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Profiler():\n",
    "    # 1\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Inicializa una instancia de la clase Profiler.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): El DataFrame que se analizar√°.\n",
    "\n",
    "        Atributos:\n",
    "            data (pd.DataFrame): El DataFrame proporcionado para el an√°lisis.\n",
    "            timestamp (str): Una marca de tiempo en el formato 'dd-mm-yyyy' que se utiliza para crear una carpeta de salida.\n",
    "            output_folder (str): El nombre de la carpeta de salida creada para el an√°lisis.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.timestamp = datetime.now().strftime('%d-%m-%Y')\n",
    "        self.output_folder = f'EDA_{self.timestamp}'\n",
    "        os.makedirs(self.output_folder, exist_ok=True)\n",
    "        \n",
    "        \n",
    "    # 2  \n",
    "    def summarize(self, variables_of_interest=None, criteria_for_outliers=None):\n",
    "        \"\"\"\n",
    "        Resumen de las estad√≠sticas y caracter√≠sticas de las variables en el DataFrame.\n",
    "\n",
    "        Genera un archivo de resumen que incluye informaci√≥n sobre el tipo de variable, n√∫mero de valores √∫nicos, \n",
    "        porcentaje de valores √∫nicos, n√∫mero de valores nulos, porcentaje de valores nulos, n√∫mero de valores cero, \n",
    "        porcentaje de valores cero, n√∫mero de valores negativos, porcentaje de valores negativos y estad√≠sticas descriptivas \n",
    "        (si la variable es num√©rica).\n",
    "\n",
    "        Args:\n",
    "            variables_of_interest (list or None): Lista de variables a incluir en el resumen. Si es None, se utilizan todas las columnas del DataFrame.\n",
    "            criteria_for_outliers (tuple or None): Criterio para considerar outliers en variables num√©ricas. \n",
    "                Debe ser un par de valores (umbral_m√≠nimo, umbral_m√°ximo). Si es None, no se calculan los outliers.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if variables_of_interest is None:\n",
    "            variables_of_interest = self.data.columns\n",
    "\n",
    "        summary_file = os.path.join(self.output_folder, 'summary.txt')\n",
    "        with open(summary_file, 'w') as file:\n",
    "            for column in variables_of_interest:\n",
    "                if column in self.data.columns:\n",
    "                    file.write(f'Variable: {column}\\n')\n",
    "                    file.write(f'Tipo de Variable: {self.data[column].dtype}\\n')\n",
    "                    file.write(f'N√∫mero de Valores √önicos: {self.data[column].nunique()}\\n')\n",
    "                    file.write(f'Porcentaje de Valores √önicos: {100 * self.data[column].nunique() / len(self.data):.2f}%\\n')\n",
    "                    file.write(f'N√∫mero de Valores Nulos: {self.data[column].isnull().sum()}\\n')\n",
    "                    file.write(f'Porcentaje de Valores Nulos: {100 * self.data[column].isnull().sum() / len(self.data):.2f}%\\n')\n",
    "\n",
    "                    if self.data[column].dtype in ['int64', 'float64']:\n",
    "                        num_zeros = (self.data[column] == 0).sum()\n",
    "                        num_negatives = (self.data[column] < 0).sum()\n",
    "                        \n",
    "                        file.write(f'N√∫mero de Valores Cero: {num_zeros}\\n')\n",
    "                        file.write(f'Porcentaje de Valores Cero: {100 * num_zeros / len(self.data):.2f}%\\n')\n",
    "                        file.write(f'N√∫mero de Valores Negativos: {num_negatives}\\n')\n",
    "                        file.write(f'Porcentaje de Valores Negativos: {100 * num_negatives / len(self.data):.2f}%\\n')\n",
    "                        \n",
    "                        # Considerar outliers si se proporciona un criterio\n",
    "                        if criteria_for_outliers is not None:\n",
    "                            data_column = self.data[column]\n",
    "                            min_threshold, max_threshold = criteria_for_outliers\n",
    "                            num_outliers = ((data_column < min_threshold) | (data_column > max_threshold)).sum()\n",
    "                            file.write(f'N√∫mero de Outliers: {num_outliers}\\n')\n",
    "                            file.write(f'Porcentaje de Outliers: {100 * num_outliers / len(self.data):.2f}%\\n')\n",
    "                    \n",
    "                        statistics = self.data[column].describe()\n",
    "                        file.write('Estad√≠sticas Descriptivas:\\n')\n",
    "                        file.write(statistics.to_string() + '\\n')\n",
    "                    \n",
    "                    file.write('\\n')\n",
    "                    \n",
    "    # 3       \n",
    "    def plot_vars(self, variables_of_interest=None, top_n_categories=5):\n",
    "        \"\"\"\n",
    "        Genera visualizaciones para variables en el DataFrame.\n",
    "\n",
    "        Este m√©todo crea visualizaciones que incluyen gr√°ficos de correlaci√≥n para variables num√©ricas, matrices de correlaci√≥n\n",
    "        de V de Cramer para variables categ√≥ricas y gr√°ficos espec√≠ficos para cada variable, ya sea num√©rica o categ√≥rica.\n",
    "\n",
    "        Args:\n",
    "            variables_of_interest (list or None): Lista de variables a incluir en las visualizaciones. Si es None, se utilizan todas las columnas del DataFrame.\n",
    "            top_n_categories (int): N√∫mero de categor√≠as principales a mostrar en gr√°ficos de variables categ√≥ricas.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        plots_folder = os.path.join(self.output_folder, 'plots')\n",
    "        os.makedirs(plots_folder, exist_ok=True)\n",
    "\n",
    "        if variables_of_interest is None:\n",
    "            variables_of_interest = self.data.columns\n",
    "\n",
    "        numeric_variables = [var for var in variables_of_interest if pd.api.types.is_numeric_dtype(self.data[var])]\n",
    "        categorical_variables = [var for var in variables_of_interest if pd.api.types.is_string_dtype(self.data[var])]\n",
    "\n",
    "        # Plot de correlaci√≥n entre variables num√©ricas\n",
    "        self.plot_correlation(numeric_variables, plots_folder)\n",
    "\n",
    "        # Plot de matriz de correlaci√≥n de V de Cramer entre variables categ√≥ricas\n",
    "        self.plot_cramers_heatmap(categorical_variables, plots_folder)\n",
    "        \n",
    "        for variable in variables_of_interest:\n",
    "            if variable in self.data.columns:\n",
    "                if pd.api.types.is_numeric_dtype(self.data[variable]):\n",
    "                    self.plot_numeric_variable(variable, plots_folder)\n",
    "                elif pd.api.types.is_string_dtype(self.data[variable]):\n",
    "                    self.plot_categorical_variable(variable, top_n_categories, plots_folder)                     \n",
    "\n",
    "    def plot_correlation(self, variables_of_interest, plots_folder):\n",
    "        \"\"\"\n",
    "        Crea un gr√°fico de correlaci√≥n entre variables num√©ricas y lo guarda como un archivo PDF.\n",
    "\n",
    "        Args:\n",
    "            variables_of_interest (list): Lista de variables num√©ricas para las que se calcular√° y representar√° la correlaci√≥n.\n",
    "            plots_folder (str): Directorio donde se guardar√° el archivo de gr√°ficos.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        correlation_matrix = self.data[variables_of_interest].corr()\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, ax=ax)\n",
    "        plt.title('Correlaci√≥n entre variables num√©ricas')\n",
    "        plot_filename = os.path.join(plots_folder, 'correlation.pdf')\n",
    "        plt.savefig(plot_filename, format='pdf')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_numeric_variable(self, variable, plots_folder):\n",
    "        \"\"\"\n",
    "        Crea un gr√°fico de densidad para una variable num√©rica y lo guarda como un archivo PDF.\n",
    "\n",
    "        Args:\n",
    "            variable (str): Nombre de la variable num√©rica que se representar√°.\n",
    "            plots_folder (str): Directorio donde se guardar√° el archivo de gr√°ficos.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Gr√°fico de densidad para variables num√©ricas usando Matplotlib\n",
    "        plt.figure()\n",
    "        plt.hist(self.data[variable], bins=30, color='blue', alpha=0.7)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title(f'Distribuci√≥n de {variable}')\n",
    "        plot_filename = os.path.join(plots_folder, f'{variable}.pdf')\n",
    "        plt.savefig(plot_filename, format='pdf')\n",
    "        plt.close()\n",
    "        \n",
    "    def plot_categorical_variable(self, variable, top_n_categories, plots_folder):\n",
    "        \"\"\"\n",
    "        Crea un histograma de las categor√≠as m√°s frecuentes para una variable categ√≥rica y lo guarda como un archivo PDF.\n",
    "\n",
    "        Args:\n",
    "            variable (str): Nombre de la variable categ√≥rica que se representar√°.\n",
    "            top_n_categories (int): N√∫mero de categor√≠as m√°s frecuentes a incluir en el histograma.\n",
    "            plots_folder (str): Directorio donde se guardar√° el archivo de gr√°ficos.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Histograma de las top N categor√≠as usando Matplotlib\n",
    "        top_categories = self.data[variable].value_counts().nlargest(top_n_categories)\n",
    "        fig = plt.figure()\n",
    "        top_categories.plot(kind='bar', color='green', alpha=0.7)\n",
    "        plt.title(f'Top {top_n_categories} Categor√≠as de {variable}')\n",
    "        plt.xticks(rotation=0)\n",
    "        plot_filename = os.path.join(plots_folder, f'{variable}.pdf')\n",
    "        plt.savefig(plot_filename, format='pdf')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    # Define the Cramer's V function\n",
    "    def cramers_V(self, var1, var2):\n",
    "        crosstab = np.array(pd.crosstab(var1, var2, rownames=None, colnames=None))  # Cross table building\n",
    "        stat = chi2_contingency(crosstab)[0]  # Keeping the test statistic of the Chi2 test\n",
    "        obs = np.sum(crosstab)  # Number of observations\n",
    "        mini = min(crosstab.shape) - 1  # Take the minimum value between the columns and the rows of the cross table\n",
    "        return np.sqrt(stat / (obs * mini))\n",
    "    \n",
    "    # Define el m√©todo create_cramers_matrix \n",
    "    def create_cramers_matrix(self, categorical_variables):\n",
    "        rows = []\n",
    "        data_encoded = self.data.copy()\n",
    "        for var1 in categorical_variables:\n",
    "            col = []\n",
    "            for var2 in categorical_variables:\n",
    "                cramers = self.cramers_V(data_encoded[var1], data_encoded[var2])  # Cramer's V test\n",
    "                col.append(round(cramers, 2))  # Keeping the rounded value of the Cramer's V\n",
    "            rows.append(col)\n",
    "        cramers_results = np.array(rows)\n",
    "        df = pd.DataFrame(cramers_results, columns=categorical_variables, index=categorical_variables)\n",
    "        return df\n",
    "\n",
    "    \n",
    "    def plot_cramers_heatmap(self, categorical_variables, plots_folder):\n",
    "        \"\"\"\n",
    "        Crea un gr√°fico de la matriz de correlaci√≥n de V de Cramer entre variables categ√≥ricas y lo guarda como un archivo PDF.\n",
    "\n",
    "        Args:\n",
    "            categorical_variables (list): Lista de variables categ√≥ricas para las que se calcular√° y representar√° la matriz de correlaci√≥n de V de Cramer.\n",
    "            plots_folder (str): Directorio donde se guardar√° el archivo de gr√°ficos.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        cramers_matrix = self.create_cramers_matrix(categorical_variables)\n",
    "        mask = np.zeros_like(cramers_matrix, dtype=bool)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "        with sns.axes_style(\"white\"):\n",
    "            ax = sns.heatmap(cramers_matrix, mask=mask, vmin=0., vmax=1, square=True)\n",
    "\n",
    "        plot_filename = os.path.join(plots_folder, 'cramers_matrix.pdf')\n",
    "        plt.savefig(plot_filename, format='pdf')\n",
    "        plt.close()\n",
    "    \n",
    "    # 4\n",
    "    def clean_data(self, variables_of_interest=None, drop_duplicates=True, drop_na=True, impute_na=False, numeric_cols = ['age', 'height', 'weight']):\n",
    "        \"\"\"\n",
    "        Limpia y preprocesa los datos del Profiler.\n",
    "\n",
    "        Args:\n",
    "            variables_of_interest (list): Lista de variables de inter√©s a ser incluidas en el resultado limpio (por defecto, todas las variables).\n",
    "            drop_duplicates (bool): Indica si se deben eliminar filas duplicadas (por defecto, True).\n",
    "            drop_na (bool): Indica si se deben eliminar filas con valores nulos (por defecto, True).\n",
    "            impute_na (bool): Indica si se deben imputar los valores nulos en las columnas (por defecto, False).\n",
    "            numeric_cols (list): Lista de columnas num√©ricas a ser convertidas (por defecto, ['age', 'height', 'weight']).\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        def custom_impute_na(X):\n",
    "            \"\"\"\n",
    "            Imputa valores nulos en un DataFrame dividiendo las columnas num√©ricas y categ√≥ricas,\n",
    "            y reemplazando los valores nulos en las columnas num√©ricas con la media de la columna y\n",
    "            los valores nulos en las columnas categ√≥ricas con la moda (valor m√°s frecuente).\n",
    "\n",
    "            Args:\n",
    "                X (DataFrame): El DataFrame que contiene las columnas a ser imputadas.\n",
    "\n",
    "            Returns:\n",
    "                DataFrame: El DataFrame resultante con los valores nulos imputados.\n",
    "            \"\"\"\n",
    "            numeric_columns = X.select_dtypes(include=[np.number])\n",
    "            categorical_columns = X.select_dtypes(exclude=[np.number])\n",
    "\n",
    "            # Imputa valores nulos en columnas num√©ricas con la media\n",
    "            numeric_columns = numeric_columns.fillna(numeric_columns.mean())\n",
    "\n",
    "            # Imputa valores nulos en columnas categ√≥ricas con la moda (valor m√°s frecuente)\n",
    "            categorical_columns = categorical_columns.fillna(categorical_columns.mode().iloc[0])\n",
    "\n",
    "            return pd.concat([numeric_columns, categorical_columns], axis=1)\n",
    "        \n",
    "        def convert_numeric_columns(X, columns):\n",
    "            \"\"\"\n",
    "            Convierte las columnas especificadas en el DataFrame a tipo num√©rico. Los valores no num√©ricos se\n",
    "            convierten en NaN (Not-a-Number).\n",
    "\n",
    "            Args:\n",
    "                X (DataFrame): El DataFrame que contiene las columnas a convertir.\n",
    "                columns (list): Una lista de nombres de columnas que se convertir√°n a tipo num√©rico.\n",
    "\n",
    "            Returns:\n",
    "                DataFrame: El DataFrame resultante con las columnas especificadas convertidas a tipo num√©rico.\n",
    "            \"\"\"\n",
    "            for column in columns:\n",
    "                X[column] = pd.to_numeric(X[column], errors='coerce')\n",
    "            return X\n",
    "        \n",
    "        \n",
    "        clean_data_folder = os.path.join(self.output_folder, 'clean_data')\n",
    "        os.makedirs(clean_data_folder, exist_ok=True)\n",
    "\n",
    "        data_to_clean = self.data.copy()\n",
    "\n",
    "        # Detecta autom√°ticamente la columna con datos no at√≥micos\n",
    "        non_atomic_column = self.detect_non_atomic_column(data_to_clean)\n",
    "\n",
    "        if non_atomic_column:\n",
    "            split_column_func = FunctionTransformer(lambda x: x.apply(self.split_column), validate=False)\n",
    "            separated_data = split_column_func.transform(data_to_clean[non_atomic_column])\n",
    "\n",
    "            # Verifica que todas las listas tengan la misma longitud\n",
    "            if separated_data.apply(len).nunique() == 1:\n",
    "                # Obtiene los elementos separados de non_atomic_column\n",
    "                column_names = non_atomic_column.split('-')\n",
    "                num_columns = len(column_names)\n",
    "\n",
    "                # Verifica que la cantidad de elementos coincida con la longitud de las listas\n",
    "                if num_columns == separated_data.apply(len).iloc[0]:\n",
    "                    separated_data = pd.DataFrame(separated_data.tolist(), columns=column_names)\n",
    "                    data_to_clean = pd.concat([data_to_clean, separated_data], axis=1)\n",
    "                    data_to_clean.drop(columns=[non_atomic_column], inplace=True)\n",
    "                else:\n",
    "                    print(\"El n√∫mero de elementos separados no coincide con la longitud de las listas.\")\n",
    "            else:\n",
    "                print(\"Las listas no tienen la misma longitud, no se pueden separar en columnas.\")\n",
    "        \n",
    "        convert_numeric_func = FunctionTransformer(convert_numeric_columns, validate=False, kw_args={'columns': numeric_cols})\n",
    "        data_to_clean = convert_numeric_func.transform(data_to_clean)\n",
    "        \n",
    "        \n",
    "        if variables_of_interest is not None:\n",
    "            data_to_clean = data_to_clean[variables_of_interest]\n",
    "\n",
    "        if drop_duplicates:\n",
    "            drop_duplicates_func = FunctionTransformer(lambda x: x.drop_duplicates(), validate=False)\n",
    "            data_to_clean = drop_duplicates_func.transform(data_to_clean)\n",
    "\n",
    "        if drop_na:\n",
    "            drop_na_func = FunctionTransformer(lambda x: x.dropna(), validate=False)\n",
    "            data_to_clean = drop_na_func.transform(data_to_clean)\n",
    "\n",
    "        if impute_na:\n",
    "            impute_na_func = FunctionTransformer(custom_impute_na, validate=False)\n",
    "            data_to_clean = impute_na_func.transform(data_to_clean)\n",
    "\n",
    "        data_to_clean.to_csv(os.path.join(clean_data_folder, 'data.csv'), index=False)\n",
    "        self.data = data_to_clean\n",
    "        \n",
    "    def split_column(self, row):\n",
    "        \"\"\"\n",
    "        Divide una columna no at√≥mica en elementos separados utilizando una expresi√≥n regular.\n",
    "\n",
    "        Args:\n",
    "            row (str): Valor de la columna no at√≥mica.\n",
    "\n",
    "        Returns:\n",
    "            list: Lista de elementos separados.\n",
    "        \"\"\"\n",
    "        # Utiliza una expresi√≥n regular para buscar patrones de separadores\n",
    "        return re.split(r'[*():?]', row)\n",
    "\n",
    "    def detect_non_atomic_column(self, data):\n",
    "        \"\"\"\n",
    "        Detecta autom√°ticamente una columna que contiene datos no at√≥micos.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): Los datos a analizar.\n",
    "\n",
    "        Returns:\n",
    "            str: El nombre de la columna con datos no at√≥micos, o None si no se encuentra.\n",
    "        \"\"\"\n",
    "        # Busca una columna que contenga caracteres especiales o separadores\n",
    "        for column in data.columns:\n",
    "            if any(char in column for char in ('-', ';', ':', '|')):\n",
    "                return column\n",
    "        return None\n",
    "    \n",
    "     # 5\n",
    "    def scale(self, cat_scaler=OneHotEncoder):\n",
    "        \"\"\"\n",
    "        Escala los datos num√©ricos y categ√≥ricos del DataFrame.\n",
    "\n",
    "        Par√°metros:\n",
    "        - cat_scaler: Una clase de preprocesador de sklearn para datos categ√≥ricos. Debe ser compatible con el\n",
    "          framework de `sklearn`. El default es `OneHotEncoder`.\n",
    "        \"\"\"\n",
    "        # Crear la carpeta 'EDA_fecha/scale'\n",
    "        scale_folder = os.path.join(self.output_folder, 'scale')\n",
    "        os.makedirs(scale_folder, exist_ok=True)\n",
    "\n",
    "        # Usar clean_data para limpiar los datos primero\n",
    "        clean_data = self.clean_data()\n",
    "\n",
    "        # Definir el escalador logar√≠tmico para transformar los datos num√©ricos\n",
    "        def log_transform(x): return np.log(x + 1)\n",
    "        log_scaler = FunctionTransformer(log_transform)\n",
    "\n",
    "        # Seleccionar las columnas num√©ricas y categ√≥ricas\n",
    "        num_cols = clean_data.select_dtypes(include=['int64', 'float64']).columns\n",
    "        cat_cols = clean_data.select_dtypes(include=['object', 'category', 'bool']).columns\n",
    "\n",
    "        # Definir el transformador de columnas para aplicar las transformaciones\n",
    "        column_transformer = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', Pipeline(steps=[('log', log_scaler), ('minmax', MinMaxScaler())]), num_cols),\n",
    "                ('cat', cat_scaler(), cat_cols)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Aplicar el ColumnTransformer\n",
    "        transformed_data = column_transformer.fit_transform(clean_data)\n",
    "\n",
    "        # Guardar las caracter√≠sticas escaladas en un archivo CSV\n",
    "        pd.DataFrame(self.scaled_features).to_csv(os.path.join(scale_folder, 'scaled_features.csv'), index=False)\n",
    "\n",
    "        return transformed_data\n",
    "    \n",
    "    \n",
    "    # 6\n",
    "    def make_clusters(self, clustering_algorithm = KMeans(), n_clusters_range=(2, 12), plot_elbow=True):\n",
    "        # Crear la carpeta para guardar los resultados de los clusters\n",
    "        clusters_folder = os.path.join(self.output_folder, 'clusters')\n",
    "        os.makedirs(clusters_folder, exist_ok=True)\n",
    "        \n",
    "        # Cargar y escalar los datos\n",
    "        transformed_data = self.scale() \n",
    "        \n",
    "        # Inicializar el n√∫mero √≥ptimo de cl√∫steres con None\n",
    "        optimal_n_clusters = None\n",
    "        \n",
    "        # Determinar el n√∫mero √≥ptimo de cl√∫steres si es necesario y el algoritmo lo soporta\n",
    "        if plot_elbow and hasattr(clustering_algorithm, 'n_clusters'):\n",
    "            elbow_model = KElbowVisualizer(clustering_algorithm, k=n_clusters_range)\n",
    "            elbow_model.fit(transformed_data)\n",
    "            elbow_model.show(outpath=os.path.join(clusters_folder, 'elbow_plot.png'))\n",
    "            optimal_n_clusters = elbow_model.elbow_value_\n",
    "            # Establecer el n√∫mero √≥ptimo de cl√∫steres en el algoritmo\n",
    "            clustering_algorithm.set_params(n_clusters=optimal_n_clusters)\n",
    "        \n",
    "        # Crear el pipeline con el algoritmo de clustering configurado\n",
    "        pipeline = Pipeline([\n",
    "            ('clustering', clustering_algorithm)\n",
    "        ])\n",
    "        \n",
    "        # Ajustar el pipeline a los datos ya escalados\n",
    "        pipeline.fit(transformed_data)\n",
    "\n",
    "        # Proyectar los datos a 2 dimensiones para visualizaci√≥n\n",
    "        pca = PCA(n_components=2)\n",
    "        reduced_data = pca.fit_transform(transformed_data)\n",
    "        \n",
    "        # Obtener las etiquetas del cluster\n",
    "        labels = pipeline['clustering'].labels_\n",
    "        \n",
    "        # Crear un DataFrame para la visualizaci√≥n y guardar los resultados\n",
    "        cluster_data = pd.DataFrame(reduced_data, columns=['PC1', 'PC2'])\n",
    "        cluster_data['Cluster'] = labels\n",
    "        cluster_data.to_csv(os.path.join(clusters_folder, 'cluster_data.csv'), index=False)\n",
    "        \n",
    "        # Graficar los cl√∫steres\n",
    "        self.plot_clusters(cluster_data, clusters_folder)\n",
    "\n",
    "        return reduced_data, cluster_data\n",
    "        \n",
    "    def plot_clusters(self, cluster_data, clusters_folder):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(cluster_data['PC1'], cluster_data['PC2'], c=cluster_data['Cluster'], cmap='viridis', marker='o')\n",
    "        plt.title('Cluster Visualization')\n",
    "        plt.xlabel('Principal Component 1')\n",
    "        plt.ylabel('Principal Component 2')\n",
    "        plt.colorbar(label='Cluster')\n",
    "        plt.savefig(os.path.join(clusters_folder, 'clusters_visualization.png'))\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    # 7\n",
    "    def detect_anomalies(self, algorithm):\n",
    "        # Crear carpeta para anomalies\n",
    "        path_anomalies = os.path.join(self.output_folder, 'anomalies')\n",
    "        os.makedirs(path_anomalies, exist_ok=True)\n",
    "\n",
    "        scale_step = FunctionTransformer(self.scale)\n",
    "\n",
    "        # Crear el pipeline con el algoritmo de detecci√≥n de anomal√≠as\n",
    "        pipeline = Pipeline(steps=[('preprocessor', scale_step),\n",
    "                                   ('anomaly_detector', algorithm)])\n",
    "\n",
    "        # Ajustar el pipeline a los datos\n",
    "        pipeline.fit(self.data)\n",
    "\n",
    "        # Detectar anomal√≠as\n",
    "        anomalies = pipeline.predict(self.data)\n",
    "\n",
    "        # Reducir la dimensionalidad para visualizaci√≥n\n",
    "        pca = PCA(n_components=2)\n",
    "        data_2d = pca.fit_transform(pipeline.named_steps['preprocessor'].transform(self.data))\n",
    "\n",
    "        # Graficar los resultados\n",
    "        plt.scatter(data_2d[:, 0], data_2d[:, 1], c=anomalies, cmap='Paired')\n",
    "        plt.title('Anomaly Detection')\n",
    "        plt.savefig(os.path.join(path_anomalies, 'anomaly_detection.pdf'))\n",
    "\n",
    "        # Agregar etiquetas de anomal√≠as al DataFrame y guardar\n",
    "        data_with_anomalies = self.data.copy()\n",
    "        data_with_anomalies['anomaly_label'] = anomalies\n",
    "        data_with_anomalies.to_csv(os.path.join(path_anomalies, 'data_anomalies.csv'), index=False)\n",
    "\n",
    "    \n",
    "    #8\n",
    "    def profile(self, data, clustering_algorithm, anomaly_detection_algorithm, scaling_technique, cluster_args={}, anomaly_args={}):\n",
    "        \"\"\"\n",
    "        Ejecuta todos los m√©todos de la clase Profiler en el orden apropiado.\n",
    "        \n",
    "        :param data: DataFrame de pandas con los datos a analizar.\n",
    "        :param clustering_algorithm: Algoritmo de clustering para usar en make_clusters.\n",
    "        :param anomaly_detection_algorithm: Algoritmo para detecci√≥n de anomal√≠as para usar en detect_anomalies.\n",
    "        :param scaling_technique: T√©cnicas de escalado para los datos num√©ricos y categ√≥ricos.\n",
    "        :param cluster_args: Argumentos adicionales para el algoritmo de clustering.\n",
    "        :param anomaly_args: Argumentos adicionales para el algoritmo de detecci√≥n de anomal√≠as.\n",
    "        \"\"\"\n",
    "        # Ejecutar el m√©todo summarize\n",
    "        summ = self.summarize(data)\n",
    "        \n",
    "        # Limpiar los datos\n",
    "        clean_data = self.clean_data(data)\n",
    "        \n",
    "        # Ejecutar el m√©todo plot_vars\n",
    "        plt_vars = self.plot_vars(data)\n",
    "        \n",
    "        # Escalar los datos\n",
    "        scaled_data = self.scale(clean_data, scaling_technique)\n",
    "        \n",
    "        # Ejecutar clustering\n",
    "        clusters = self.make_clusters(scaled_data, clustering_algorithm, **cluster_args)\n",
    "        \n",
    "        # Detectar anomal√≠as\n",
    "        anomalies = self.detect_anomalies(scaled_data, anomaly_detection_algorithm, **anomaly_args)\n",
    "\n",
    "        return summ, plt_vars, clean_data, scaled_data, clusters, anomalies\n",
    "\n",
    "\n",
    "    # 9\n",
    "    def clearGarbage(self):\n",
    "        \"\"\"\n",
    "        Elimina todas las carpetas y archivos creados por la clase Profiler. \n",
    "        \"\"\"\n",
    "        if os.path.exists(self.output_folder):\n",
    "            shutil.rmtree(self.output_folder)\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler = Profiler(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler.clearGarbage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Caracterizar datos de Olimpiadas (2.0 puntos)\n",
    "\n",
    "A partir de la clase que hemos desarrollado previamente, procederemos a realizar un an√°lisis exhaustivo de los datos proporcionados en el enunciado. Este an√°lisis se presentar√° en forma de un informe contenido en el mismo Jupyter Notebook y abordar√° los siguientes puntos:\n",
    "\n",
    "1. Introducci√≥n\n",
    "    - Se proporcionar√° una breve descripci√≥n del problema que estamos abordando y se explicar√° la metodolog√≠a que se seguir√°.\n",
    "\n",
    "Elaborar una breve introducci√≥n con todo lo necesario para entender qu√© realizar√°n durante su proyecto. La idea es que describan de manera formal el proyecto con sus propias palabras y logren describir algunos aspectos b√°sicos tanto del dataset como del an√°lisis a realizar sobre los datos.\n",
    "\n",
    "Por lo anterior, en esta secci√≥n ustedes deber√°n ser capaces de:\n",
    "\n",
    "- Describir la tarea asociada al dataset.\n",
    "- Describir brevemente los datos de entrada que les provee el problema.\n",
    "- Plantear hip√≥tesis de c√≥mo podr√≠an abordar el problema.\n",
    "\n",
    "2. An√°lisis del EDA (An√°lisis Exploratorio de Datos)\n",
    "    - Se discutir√°n las observaciones y conclusiones obtenidas acerca de los datos proporcionados. A lo largo de su respuesta, debe responder preguntas como:\n",
    "        - ¬øComo se comportan las variables num√©ricas? ¬øy las categ√≥ricas?\n",
    "        - ¬øExisten valores nulos en el dataset? ¬øEn qu√© columnas? ¬øCuantos?\n",
    "        - ¬øCu√°les son las categor√≠as y frecuencias de las variables categ√≥ricas?\n",
    "        - ¬øExisten datos duplicados en el conjunto?\n",
    "        - ¬øExisten relaciones o patrones visuales entre las variables?\n",
    "        - ¬øExisten anomal√≠as notables o preocupantes en los datos?\n",
    "3. Creaci√≥n de Clusters y Anomal√≠as\n",
    "    - Se justificar√° la elecci√≥n de los algoritmos a utilizar y sus hiperpar√°metros. En el caso de clustering, justifique adem√°s el n√∫mero de clusters.\n",
    "    \n",
    "4. An√°lisis de Resultados\n",
    "    - Se examinar√°n los resultados obtenidos a partir de los cl√∫sters y anomal√≠as generadas. ¬øSe logra una separaci√≥n efectiva de los datos? Entregue una interpretaci√≥n de lo que representa cada cl√∫ster y anomal√≠a.\n",
    "5. Conclusi√≥n\n",
    "    - Se resumir√°n las principales conclusiones del an√°lisis y se destacar√°n las implicaciones pr√°cticas de los resultados obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
